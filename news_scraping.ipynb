{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bea0c5",
   "metadata": {},
   "source": [
    "Get packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "967ab5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import quote\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from textblob import TextBlob\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa32d2",
   "metadata": {},
   "source": [
    "Web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6e7ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(query, num_results):\n",
    "    rss = f\"https://news.google.com/rss/search?q={quote(str(query))}\"\n",
    "    feed = feedparser.parse(rss)\n",
    "    news = feed.entries[:num_results]\n",
    "\n",
    "    articles = []\n",
    "    for entry in news:\n",
    "        title = entry.title\n",
    "        published = entry.published\n",
    "        link = entry.link\n",
    "        content = get_content(link)\n",
    "        articles.append({\"title\": title, \"published\": published, \"link\": link, \"content\": content})\n",
    "\n",
    "    return articles\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        # Get content from the URL and make content operable using soup\n",
    "        response = requests.get(url, timeout=10, allow_redirects=True)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # find all paragraph tags and headline tags <h1> and <p>\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        # Join all paragraphs text\n",
    "        text = \" \".join(p.get_text() for p in paragraphs).strip()\n",
    "\n",
    "        return text\n",
    "    except requests.RequestException:\n",
    "        return \"content not available due to request error\"\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7ad23",
   "metadata": {},
   "source": [
    "Get Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e55f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", return_all_scores=True)\n",
    "def sentiment_analysis(text):\n",
    "    # Skip if input is not a string\n",
    "    if not isinstance(text, str):\n",
    "        return {\"error\": \"Invalid content\"}\n",
    "    \n",
    "    results = pipe(text)\n",
    "    sentiments = {results['label']: results['score']}\n",
    "    return sentiments\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48679f",
   "metadata": {},
   "source": [
    "Define keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a4bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"\"\n",
    "keywords = [\"earnings\", \"revenue\", \"profit\", \"loss\"]\n",
    "query_target = [ticker + keyword for keyword in keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f4368",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73f7962a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Nebius Reports Bigger Q3 Net Income Loss, Announces Meta AI Deal - Investor's Business Daily\",\n",
       " 'published': 'Tue, 11 Nov 2025 13:18:00 GMT',\n",
       " 'link': 'https://news.google.com/rss/articles/CBMiiwFBVV95cUxNLWQ5WVNLeEZKemhqU1g2cHdhQjFFeTM2cmpBVXhSLTBNWWtuNUZHazdqSE9MZjkzaTVCZE50MzRMNm9FekFvMnVVZkp3RkEtUWxSeGw2TWZ4U2syRWVuOFA4Tl9FazJGZ2FWM2lvSzBIUkhrclZoNlF0UXNScERRcEhBWU5qTEMxeEhz?oc=5',\n",
       " 'content': ''}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = get_news(query_target, 10)\n",
    "articles[0]\n",
    "# for article in articles:\n",
    "#     sentiment = sentiment_analysis(article['content'])\n",
    "#     article['sentiment'] = sentiment\n",
    "\n",
    "# for article in articles:\n",
    "#     for k,v in article.items():\n",
    "#         print(f\"{k}: {v}\")\n",
    "#         print(\"-\" * 40)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536a264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
